"""
Date: 2021-06-11 13:54:00
LastEditors: GodK
LastEditTime: 2021-07-19 21:53:18
"""
import os
import config
import sys
import torch
import json
from transformers import BertTokenizerFast, BertModel
from models.GlobalPointer import DataMaker, MyDataset, GlobalPointer, MetricsCalculator
from torch.utils.data import DataLoader, Dataset
import numpy as np

import time

from dataclasses import dataclass, field
from typing import Optional

from transformers import (
    AutoConfig,
    AutoTokenizer,
    HfArgumentParser,
    TrainingArguments,
    AutoModel
)
from common.utils import DocumentLoader


# LayoutLMv3 Config
@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
    """

    model_name_or_path: str = field(
        default="microsoft/layoutlmv3-base", metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
    )
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    model_revision: str = field(
        default="main",
        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
    )
    use_auth_token: bool = field(
        default=False,
        metadata={
            "help": "Will use the token generated when running `transformers-cli login` (necessary to use this script "
            "with private models)."
        },
    )


@dataclass
class DataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """

    task_name: Optional[str] = field(default="ner", metadata={"help": "The name of the task (ner, pos...)."})
    dataset_name: Optional[str] = field(
        default='funsd', metadata={"help": "The name of the dataset to use (via the datasets library)."}
    )
    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
    )
    train_file: Optional[str] = field(
        default=None, metadata={"help": "The input training data file (a csv or JSON file)."}
    )
    validation_file: Optional[str] = field(
        default=None,
        metadata={"help": "An optional input evaluation data file to evaluate on (a csv or JSON file)."},
    )
    test_file: Optional[str] = field(
        default=None,
        metadata={"help": "An optional input test data file to predict on (a csv or JSON file)."},
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "The number of processes to use for the preprocessing."},
    )
    pad_to_max_length: bool = field(
        default=True,
        metadata={
            "help": "Whether to pad all samples to model maximum sentence length. "
            "If False, will pad the samples dynamically when batching to the maximum length in the batch. More "
            "efficient on GPU but very bad for TPU."
        },
    )
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
            "value if set."
        },
    )
    max_val_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": "For debugging purposes or quicker training, truncate the number of validation examples to this "
            "value if set."
        },
    )
    max_test_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": "For debugging purposes or quicker training, truncate the number of test examples to this "
            "value if set."
        },
    )
    label_all_tokens: bool = field(
        default=False,
        metadata={
            "help": "Whether to put the label for one word on all tokens of generated by that word or just on the "
            "one (in which case the other tokens will have a padding index)."
        },
    )
    return_entity_level_metrics: bool = field(
        default=False,
        metadata={"help": "Whether to return all the entity levels during evaluation or just the overall ones."},
    )
    segment_level_layout: bool = field(default=True)
    visual_embed: bool = field(default=True)
    data_dir: Optional[str] = field(default=None)
    input_size: int = field(default=224, metadata={"help": "images input size for backbone"})
    second_input_size: int = field(default=112, metadata={"help": "images input size for discrete vae"})
    train_interpolation: str = field(
        default='bicubic', metadata={"help": "Training interpolation (random, bilinear, bicubic)"})
    second_interpolation: str = field(
        default='lanczos', metadata={"help": "Interpolation for discrete vae (random, bilinear, bicubic)"})
    imagenet_default_mean_and_std: bool = field(default=False, metadata={"help": ""})


config = config.eval_config
hyper_parameters = config["hyper_parameters"]

os.environ["TOKENIZERS_PARALLELISM"] = "true"
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
config["num_workers"] = 6 if sys.platform.startswith("linux") else 0

# for reproductivity
torch.backends.cudnn.deterministic = True

def load_data(data_path, data_type="predict"):
    if data_type == "predict":
        datas = []
        with open(data_path, encoding="utf-8") as f:
            for line in f:
                line = json.loads(line)
                datas.append(line)
        return datas
    else:
        return json.load(open(data_path, encoding="utf-8"))


ent2id_path = os.path.join(config["data_home"], config["exp_name"], config["ent2id"])
ent2id = load_data(ent2id_path, "ent2id")
ent_type_size = len(ent2id)

max_seq_len = hyper_parameters["max_seq_len"]

# set up tokenizer
bert_tokenizer = BertTokenizerFast.from_pretrained(config["bert_path"], add_special_tokens=True, do_lower_case=False)

tokenizer = AutoTokenizer.from_pretrained(
        # "microsoft/layoutlmv3-base-chinese",
        "microsoft/layoutlmv3-base",  # input should be a list of tokens
        tokenizer_file=None,  # avoid loading from a cached file of the pre-trained model in another machine
        use_fast=True,
        add_prefix_space=True
    )


def load_document_data(documentloader, data_type='train', use_image=False):
    documents = documentloader.load_document_list(data_type)

    # original GP
    ent_type_size = len(ent2id)  # 实体类别

    all_inputs = []
    for i, sample in enumerate(documents):
        # {'input_ids', 'attention_msk', 'overflow_to_sample_mapping', 'bbox', 'labels', 'images'}
        tokenized_inputs = documentloader.tokenize_and_align_labels(sample[1], ent2id, max_seq_len=max_seq_len, img_id=i)  # {'input_ids', 'attention_msk', 'overflow_to_sample_mapping', 'bbox', 'labels', 'images'}

        input_ids = torch.tensor(tokenized_inputs["input_ids"]).long()
        attention_mask = torch.tensor(tokenized_inputs["attention_mask"]).long()
        bbox = torch.tensor(tokenized_inputs["bbox"])  # (N, 4) with top-left, right-bottom
        
        if tokenized_inputs['labels'] is not None:
            labels = torch.tensor(tokenized_inputs["labels"]).long()

        images = None
        if use_image:
            images = torch.unsqueeze(tokenized_inputs['images'][0], dim=0)

        # sample_input = (sample, input_ids, attention_mask, token_type_ids, labels)
        sample_input = (sample, input_ids, attention_mask, bbox, images, labels)

        all_inputs.append(sample_input)

    return all_inputs


def data_generator(data_type="predict", exp_name='FUNSD-r'):
    """
    读取数据，生成DataLoader。
    """

    if exp_name == 'FUNSD-r':
        documentloader = DocumentLoader(tokenizer)
        if data_type == 'predict':
            predict_data = load_document_data(documentloader, data_type='test')
        elif data_type == 'temp':
            predict_data = load_document_data(documentloader, data_type='temp2')
        print('check : ', len(predict_data))
    else:
        if data_type == "predict":
            predict_data_path = os.path.join(config["data_home"], config["exp_name"], config["predict_data"])
            predict_data = load_data(predict_data_path, "predict")

        all_data = predict_data

        # TODO:句子截取
        max_tok_num = 0
        for sample in all_data:
            tokens = tokenizer.tokenize(sample["text"])
            max_tok_num = max(max_tok_num, len(tokens))
        assert max_tok_num <= hyper_parameters[
            "max_seq_len"], f'数据文本最大token数量{max_tok_num}超过预设{hyper_parameters["max_seq_len"]}'
        # max_seq_len = min(max_tok_num, hyper_parameters["max_seq_len"])

    data_maker = DataMaker(tokenizer)

    if data_type == "predict":
        predict_dataloader = DataLoader(MyDataset(predict_data),
                                     batch_size=hyper_parameters["batch_size"],
                                     shuffle=False,
                                     num_workers=config["num_workers"],
                                     drop_last=False,
                                     collate_fn=lambda x: data_maker.generate_batch(x, max_seq_len, ent2id,
                                                                                    data_type="predict")
                                     )
        return predict_dataloader


def decode_ent(text, pred_matrix, tokenizer, threshold=0):
    # print(text)
    token2char_span_mapping = tokenizer(text, return_offsets_mapping=True)["offset_mapping"]
    id2ent = {id: ent for ent, id in ent2id.items()}
    pred_matrix = pred_matrix.cpu().numpy()
    ent_list = {}
    for ent_type_id, token_start_index, token_end_index in zip(*np.where(pred_matrix > threshold)):
        ent_type = id2ent[ent_type_id]
        ent_char_span = [token2char_span_mapping[token_start_index][0], token2char_span_mapping[token_end_index][1]]
        ent_text = text[ent_char_span[0]:ent_char_span[1]]

        ent_type_dict = ent_list.get(ent_type, {})
        ent_text_list = ent_type_dict.get(ent_text, [])
        ent_text_list.append(ent_char_span)
        ent_type_dict.update({ent_text: ent_text_list})
        ent_list.update({ent_type: ent_type_dict})
    # print(ent_list)
    return ent_list


def predict(dataloader, model):
    predict_res = []

    model.eval()
    for batch_data in dataloader:
        batch_samples, batch_input_ids, batch_attention_mask, batch_token_type_ids, _ = batch_data
        batch_input_ids, batch_attention_mask, batch_token_type_ids = (batch_input_ids.to(device),
                                                                       batch_attention_mask.to(device),
                                                                       batch_token_type_ids.to(device),
                                                                       )
        with torch.no_grad():
            batch_logits = model(batch_input_ids, batch_attention_mask, batch_token_type_ids)

        for ind in range(len(batch_samples)):
            gold_sample = batch_samples[ind]
            text = gold_sample["text"]
            text_id = gold_sample["id"]
            pred_matrix = batch_logits[ind]
            labels = decode_ent(text, pred_matrix, tokenizer)
            predict_res.append({"id": text_id, "text": text, "label": labels})
    return predict_res


def load_model():
    model_state_dir = config["model_state_dir"]
    model_state_list = sorted(filter(lambda x: "model_state" in x, os.listdir(model_state_dir)),
                              key=lambda x: int(x.split(".")[0].split("_")[-1]))
    last_k_model = config["last_k_model"]
    
    model_state_path = os.path.join(model_state_dir, model_state_list[-last_k_model])
    print(model_state_path)
    print('**********************************')

    # encoder = BertModel.from_pretrained(config["bert_path"])

    # LayoutLMv3
    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))

    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    layoutlmconfig = AutoConfig.from_pretrained(
        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
        # "microsoft/layoutlmv3-large",
        # "microsoft/layoutlmv3-base-chinese",
        num_labels=4,
        finetuning_task=data_args.task_name,
        # cache_dir=model_args.cache_dir,
        revision=model_args.model_revision,
        input_size=data_args.input_size,
        use_auth_token=True if model_args.use_auth_token else None,
    )

    encoder = AutoModel.from_pretrained(
        model_args.model_name_or_path,
        # "microsoft/layoutlmv3-large",
        # "microsoft/layoutlmv3-base-chinese",
        from_tf=bool(".ckpt" in model_args.model_name_or_path),
        config=layoutlmconfig,
        # cache_dir=model_args.cache_dir,
        revision=model_args.model_revision,
        use_auth_token=True if model_args.use_auth_token else None,
    )
    model = GlobalPointer(encoder, ent_type_size, 64)
    model.load_state_dict(torch.load(model_state_path))
    model = model.to(device)
    print('Eval model : ', model_args.model_name_or_path)
    return model


def evaluate():
    predict_dataloader = data_generator(data_type="predict", exp_name=config['exp_name'])

    model = load_model()

    predict_res = predict(predict_dataloader, model)

    if not os.path.exists(os.path.join(config["save_res_dir"], config["exp_name"])):
        os.mkdir(os.path.join(config["save_res_dir"], config["exp_name"]))
    save_path = os.path.join(config["save_res_dir"], config["exp_name"], "predict_result.json")
    # json.dump(predict_res, open(save_path, "w", encoding="utf-8"), ensure_ascii=False)
    with open(save_path, "w", encoding="utf-8") as f:
        for item in predict_res:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")


if __name__ == '__main__':
    evaluate()
